{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Clarity Tutorial - simple python code search example \n",
    "In this notebook we hope to go through a simple example of how to use codeclarity to generate vectors of source\n",
    "code and natrual language to build a simple vector retrieval application to power a very simple search engine. \n",
    "\n",
    "## Through the course of this notebook, we will cover-\n",
    "1. The basics of semantic search\n",
    "2. The applications of dense vectors for information retrieval\n",
    "3. A model that was finetuned for code search of python-english source code. \n",
    "4. Implimenting a simple search of an example large repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal : semantic search of code \n",
    "\n",
    "To quote from Neil Reimers project [sentence transformers](https://www.sbert.net/examples/applications/semantic-search/README.html), which is an excellent resource for learning about dense vectors and their uses in NLP at large:\n",
    "\n",
    "\"*Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines which only find documents based on lexical matches, semantic search can also find synonyms.*\"\n",
    "\n",
    "\n",
    "This means that semantic search differs from it's predecessor, lexical search, in that instead of looking specific patterns that match in a retrieval object such as keyword matching, we aim to use machine learning models to capture a deeper contextual meaning of objects that we can map to a textual description- namely a query.\n",
    "\n",
    " This is an idea that has been adopted to great success to power every major search engine in the past 5 years, and has the benefit of being able to power search for arbitary sequences of symbols, be them [text](https://www.sbert.net/docs/pretrained_models.html), [images](https://openai.com/blog/clip/) or [audio](https://arxiv.org/abs/1904.05073).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of dense vectors\n",
    "### Sparse VS Dense Vectors\n",
    "Before the common prevelence of deep learning models in natrual language processing across machine learning, before the use of  release of [BERT](https://arxiv.org/abs/1706.03762) and [Word2Vec Varients](https://arxiv.org/abs/1301.3781), a common mathmatical representation of textual features was [TF-IDF](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3): the Term Frequency Inverse Document Frequency of a sequence.\n",
    "\n",
    " We won't explain it here, but TF-IDF is an example of a sparse vector representation. There are as many elements in the array storing the representation as there are unique words in the training corpus. This being said, a given piece of text, especially if it has a small vocabulary, will consist of mostly zeros. \n",
    "\n",
    " This contrasts with for example, a [S-BERT sentence embedding](https://www.sbert.net/examples/applications/computing-embeddings/README.html), where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print \"Hello World\"\n",
    "```\n",
    "\n",
    "```javascript\n",
    "console.log(\"Hello World\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
